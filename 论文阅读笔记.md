对于一个表格T，可以看作许多kv的集合, 每一个kv是一个record $R_i$

它的v有一系列word组成，$d_1,d_2,... d_m$, 相应的field embedding $Z_{d1},Z_{d2},...Z_{dn}$

输出的句子有P个token $w_1,w_2,...w_n$

将句子生成的问题变成概率模型的问题，目标是生成一个句子 $w^*_{1:p}$, 要最大化似然函数 $P(w_{1:p}|R_{1:n})$

![image-20220607130733515](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071307587.png)



文中用到了两种embedding

+ context embedding：embedding for a  segment of words in the field content
+ filed embeding： 一个三元组 $(field, postion^+,postion^-)$，$postion^+$指的是从左往右数，单词的位置![image-20220607133509746](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071335830.png)



编码器上提出了一个新的架构 Field-gating Table Encoder，就是多加了一个门来处理field的信息

传统LSTM：$d_t$是word embedding

![image-20220607133958888](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071339932.png)

加了Filed-gating：$z_t$是前面的field embedding

![image-20220607134105643](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071341680.png)

![在这里插入图片描述](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071345013.jpeg)



decoder 部分用了一个dual attention

传统seq2seq的attention只关注输出和输入token之间的关系，忽略了表格的结构信息

他这里加了一个field-level attention，本质上就是将之前的field embedding 和 输出再做一次attention

$B_{ti}$是权重，$g(s_t,z_i)$是相关函数，最终的attention的权重，有encoder的attention 权重和 field level的attention的权重共同给出

![image-20220607140820062](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071408106.png)

对生成的\<UNK\> token，使用注意力分数中相关度最高的token替换

 总结下来，整体网络架构如下：编码的时候加word-embedding和field-embedding一起加进去，然后两个分别做attention，得到权重，在做一次softmax，得到最后的权重，最后得到docoder位置的值

![image-20220607141340709](https://raw.githubusercontent.com/shadow150519/PictureBed/main/202206071413800.png)
